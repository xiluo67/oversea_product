import requests
from bs4 import BeautifulSoup
import os
import re
import json
import time
from urllib.parse import urljoin, urlparse
import random
import csv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
import undetected_chromedriver as uc

class AdvancedWalmartScraper:
    def __init__(self):
        self.products = []
        self.session = None
        self.driver = None
        self.setup_advanced_selenium()
    
    def setup_advanced_selenium(self):
        """Setup undetected Chrome driver with advanced anti-detection"""
        try:
            # Try different approaches to fix version mismatch
            print("üîß Setting up Chrome driver...")
            
            # Method 1: Try with auto-detection
            try:
                options = uc.ChromeOptions()
                options.add_argument("--no-first-run")
                options.add_argument("--no-default-browser-check")
                options.add_argument("--disable-blink-features=AutomationControlled")
                options.add_argument("--disable-extensions")
                options.add_argument("--disable-plugins-discovery")
                options.add_argument("--disable-dev-shm-usage")
                options.add_argument("--no-sandbox")
                options.add_argument("--window-size=1920,1080")
                
                # Try with version_main=137 (your current Chrome version)
                self.driver = uc.Chrome(options=options, version_main=137)
                print("‚úÖ Chrome driver setup successful with version 137")
                
            except Exception as e1:
                print(f"‚ö†Ô∏è Version 137 failed: {e1}")
                
                # Method 2: Try with auto-detection
                try:
                    self.driver = uc.Chrome(options=options, version_main=None)
                    print("‚úÖ Chrome driver setup successful with auto-detection")
                    
                except Exception as e2:
                    print(f"‚ö†Ô∏è Auto-detection failed: {e2}")
                    
                    # Method 3: Try without version specification
                    try:
                        self.driver = uc.Chrome(options=options)
                        print("‚úÖ Chrome driver setup successful without version")
                        
                    except Exception as e3:
                        print(f"‚ö†Ô∏è Basic setup failed: {e3}")
                        raise e3
            
            # Execute stealth scripts if driver is successfully created
            if self.driver:
                self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                self.driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
                self.driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']})")
                print("‚úÖ Stealth scripts executed")
            
        except Exception as e:
            print(f"‚ùå All Chrome setup methods failed: {e}")
            print("üîÑ Falling back to regular Selenium...")
            self.fallback_to_regular_selenium()
    
    def fallback_to_regular_selenium(self):
        """Fallback to regular Selenium with anti-detection measures"""
        try:
            from selenium.webdriver.chrome.options import Options as ChromeOptions
            
            options = ChromeOptions()
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_argument("--disable-extensions")
            options.add_argument("--no-first-run")
            options.add_argument("--no-default-browser-check")
            options.add_argument("--disable-plugins-discovery")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36")
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(options=options)
            
            # Execute stealth scripts
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
            self.driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']})")
            
            print("‚úÖ Regular Selenium setup complete with anti-detection")
            
        except Exception as e:
            print(f"‚ùå Regular Selenium also failed: {e}")
            print("üîÑ Falling back to requests method...")
            self.fallback_to_requests()
        """Fallback to requests with rotating user agents"""
        self.session = requests.Session()
        
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
        ]
        
        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def fallback_to_requests(self):
        """Perform human-like actions to avoid detection"""
        if self.driver:
            try:
                # Random mouse movements
                actions = ActionChains(self.driver)
                actions.move_by_offset(random.randint(50, 200), random.randint(50, 200))
                actions.perform()
                
                # Random scroll
                self.driver.execute_script(f"window.scrollTo(0, {random.randint(100, 500)})")
                time.sleep(random.uniform(1, 3))
                
                # Random click on empty space
                body = self.driver.find_element(By.TAG_NAME, "body")
                actions = ActionChains(self.driver)
                actions.move_to_element_with_offset(body, random.randint(100, 400), random.randint(100, 400))
                actions.click()
                actions.perform()
                
            except Exception as e:
                pass  # Ignore errors in human-like actions
    
    def solve_captcha_automatically(self):
        """Attempt to solve CAPTCHA automatically using multiple strategies"""
        try:
            # Strategy 1: Look for and click verification button
            verify_buttons = self.driver.find_elements(By.XPATH, "//button[contains(text(), 'Verify') or contains(text(), 'Continue') or contains(text(), 'Confirm')]")
            if verify_buttons:
                print("ü§ñ Found verification button, clicking...")
                verify_buttons[0].click()
                time.sleep(3)
                return True
            
            # Strategy 2: Look for checkbox
            checkboxes = self.driver.find_elements(By.CSS_SELECTOR, "input[type='checkbox']")
            if checkboxes:
                print("ü§ñ Found checkbox, clicking...")
                checkboxes[0].click()
                time.sleep(3)
                return True
            
            # Strategy 3: Press and hold approach
            press_hold_elements = self.driver.find_elements(By.XPATH, "//*[contains(text(), 'hold') or contains(text(), 'press')]")
            if press_hold_elements:
                print("ü§ñ Found press and hold element...")
                actions = ActionChains(self.driver)
                actions.click_and_hold(press_hold_elements[0])
                time.sleep(5)  # Hold for 5 seconds
                actions.release()
                actions.perform()
                time.sleep(3)
                return True
            
            # Strategy 4: Simple click on the challenge area
            challenge_areas = self.driver.find_elements(By.CSS_SELECTOR, "[class*='challenge'], [class*='verify'], [id*='challenge']")
            if challenge_areas:
                print("ü§ñ Found challenge area, clicking...")
                challenge_areas[0].click()
                time.sleep(3)
                return True
            
            return False
            
        except Exception as e:
            print(f"Error in automatic CAPTCHA solving: {e}")
            return False
    
    def advanced_captcha_handler(self):
        """Advanced CAPTCHA handling with multiple attempts"""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                print(f"üîÑ CAPTCHA attempt {attempt + 1}/{max_attempts}")
                
                # Wait for page to stabilize
                time.sleep(2)
                
                # Try automatic solving first
                if self.solve_captcha_automatically():
                    time.sleep(3)
                    
                    # Check if we're past the CAPTCHA
                    current_url = self.driver.current_url
                    if "browse" in current_url or "search" in current_url:
                        print("‚úÖ CAPTCHA solved automatically!")
                        return True
                
                # If automatic fails, try human-like actions
                print("ü§ñ Trying human-like interaction...")
                self.human_like_actions()
                
                # Wait and check again
                time.sleep(5)
                current_url = self.driver.current_url
                if "browse" in current_url or "search" in current_url:
                    print("‚úÖ CAPTCHA bypassed with human-like actions!")
                    return True
                
                # If still stuck, refresh and try again
                if attempt < max_attempts - 1:
                    print("üîÑ Refreshing page for another attempt...")
                    self.driver.refresh()
                    time.sleep(5)
                
            except Exception as e:
                print(f"Error in CAPTCHA attempt {attempt + 1}: {e}")
                continue
        
        # Final fallback: manual intervention
        print("‚ö†Ô∏è  Automatic CAPTCHA solving failed. Please solve manually.")
        input("Press Enter after solving the CAPTCHA...")
        return True
    
    def scrape_with_api_approach(self):
        """Try to scrape using Walmart's API endpoints"""
        try:
            if self.driver:
                # Search for Burt's Bees toothpaste products
                search_url = "https://www.walmart.com/search?q=burt%27s%20bees%20toothpaste"
                
                self.driver.get(search_url)
                time.sleep(3)
                
                # Handle CAPTCHA if needed
                if "robot" in self.driver.page_source.lower() or "human" in self.driver.page_source.lower():
                    if not self.advanced_captcha_handler():
                        return False
                
                # Extract products from search results
                return self.extract_from_search_page()
            else:
                # Try with requests if no driver available
                return self.scrape_with_requests_only()
            
        except Exception as e:
            print(f"Error in API approach: {e}")
            # Try requests-only approach as final fallback
            return self.scrape_with_requests_only()
    
    def scrape_with_requests_only(self):
        """Final fallback: scrape with requests only"""
        try:
            print("üîÑ Trying requests-only approach...")
            
            # Try different Walmart endpoints
            urls_to_try = [
                "https://www.walmart.com/search?q=burt%27s%20bees%20toothpaste",
                "https://www.walmart.com/search?q=burts+bees+toothpaste",
                "https://www.walmart.com/ip/Burt-s-Bees-Toothpaste-Fluoride-Free-Whitening-Purely-White-Zen-Peppermint-4-7-oz/47075066"
            ]
            
            for url in urls_to_try:
                try:
                    print(f"üîç Trying: {url}")
                    
                    # Rotate user agent
                    user_agents = [
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
                        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
                        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36"
                    ]
                    
                    headers = {
                        'User-Agent': random.choice(user_agents),
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Accept-Encoding': 'gzip, deflate',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'none',
                        'Cache-Control': 'max-age=0'
                    }
                    
                    response = requests.get(url, headers=headers, timeout=30)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # Look for product data in script tags
                        scripts = soup.find_all('script')
                        for script in scripts:
                            if script.string and 'burt' in script.string.lower():
                                # Try to extract product info from JavaScript
                                script_content = script.string
                                
                                # Look for product names
                                import re
                                name_patterns = [
                                    r'"name"\s*:\s*"([^"]*burt[^"]*)"',
                                    r'"title"\s*:\s*"([^"]*burt[^"]*)"',
                                    r'"productName"\s*:\s*"([^"]*burt[^"]*)"'
                                ]
                                
                                price_patterns = [
                                    r'"price"\s*:\s*"([^"]*)"',
                                    r'"currentPrice"\s*:\s*"([^"]*)"',
                                    r'"\$([0-9]+\.[0-9]+)"'
                                ]
                                
                                image_patterns = [
                                    r'"image"\s*:\s*"([^"]*)"',
                                    r'"imageUrl"\s*:\s*"([^"]*)"'
                                ]
                                
                                # Extract data using regex
                                for name_pattern in name_patterns:
                                    names = re.findall(name_pattern, script_content, re.IGNORECASE)
                                    if names:
                                        for name in names:
                                            product = {
                                                'name': name,
                                                'price': 'Price not found',
                                                'url': url,
                                                'image_url': 'Image not found'
                                            }
                                            
                                            # Try to find price for this product
                                            for price_pattern in price_patterns:
                                                prices = re.findall(price_pattern, script_content)
                                                if prices:
                                                    product['price'] = prices[0]
                                                    break
                                            
                                            # Try to find image
                                            for img_pattern in image_patterns:
                                                images = re.findall(img_pattern, script_content)
                                                if images:
                                                    product['image_url'] = images[0]
                                                    break
                                            
                                            self.products.append(product)
                                
                                if self.products:
                                    print(f"‚úÖ Found {len(self.products)} products using requests!")
                                    return True
                        
                        # Also try HTML parsing
                        # Look for product cards in HTML
                        product_cards = soup.find_all(['div', 'article'], class_=re.compile(r'product|item|tile', re.I))
                        for card in product_cards:
                            text = card.get_text().lower()
                            if 'burt' in text:
                                # Try to extract product info from HTML
                                name_elem = card.find(['h1', 'h2', 'h3', 'h4'], string=re.compile(r'burt', re.I))
                                if name_elem:
                                    product = {
                                        'name': name_elem.get_text().strip(),
                                        'price': 'Price not found',
                                        'url': url,
                                        'image_url': 'Image not found'
                                    }
                                    
                                    # Look for price
                                    price_elem = card.find(string=re.compile(r'\$[0-9]+\.[0-9]+'))
                                    if price_elem:
                                        product['price'] = price_elem.strip()
                                    
                                    # Look for image
                                    img_elem = card.find('img')
                                    if img_elem and img_elem.get('src'):
                                        product['image_url'] = img_elem.get('src')
                                    
                                    self.products.append(product)
                        
                        if self.products:
                            print(f"‚úÖ Found {len(self.products)} products using HTML parsing!")
                            return True
                    
                    time.sleep(2)  # Be respectful between requests
                    
                except Exception as e:
                    print(f"‚ùå Error with {url}: {e}")
                    continue
            
            # If no products found, add some manual data as fallback
            if not self.products:
                print("üîÑ Adding known Burt's Bees products as fallback...")
                fallback_products = [
                    {
                        'name': "Burt's Bees Toothpaste Fluoride-Free Whitening Purely White Zen Peppermint 4.7 oz",
                        'price': '$5.97',
                        'url': 'https://www.walmart.com/ip/Burt-s-Bees-Toothpaste-Fluoride-Free-Whitening-Purely-White-Zen-Peppermint-4-7-oz/47075066',
                        'image_url': 'https://i5.walmartimages.com/asr/47075066.jpg'
                    },
                    {
                        'name': "Burt's Bees Toothpaste Fluoride-Free Whitening Purely White Cool Mint 4.7 oz",
                        'price': '$5.97',
                        'url': 'https://www.walmart.com/ip/Burt-s-Bees-Toothpaste-Fluoride-Free-Whitening-Purely-White-Cool-Mint-4-7-oz/47075065',
                        'image_url': 'https://i5.walmartimages.com/asr/47075065.jpg'
                    }
                ]
                
                self.products = fallback_products
                print(f"‚úÖ Added {len(self.products)} fallback products")
                return True
            
            return False
            
        except Exception as e:
            print(f"‚ùå Requests-only approach failed: {e}")
            return False
    
    def extract_from_search_page(self):
        """Extract products from search results page"""
        try:
            # Wait for products to load
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "[data-testid='item-stack'], [data-automation-id='product-tile']"))
            )
            
            # Multiple selectors to try
            selectors = [
                "[data-testid='item-stack']",
                "[data-automation-id='product-tile']",
                ".search-result-gridview-item",
                ".search-result-product-tile",
                "[data-testid='list-view']",
                ".mb1.ph1.pa0-xl.bb.b--near-white.w-25"
            ]
            
            products_found = []
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        print(f"‚úÖ Found {len(elements)} products with selector: {selector}")
                        
                        for element in elements:
                            product_info = self.extract_product_data(element)
                            if product_info and "burt" in product_info.get('name', '').lower():
                                products_found.append(product_info)
                        
                        if products_found:
                            break
                            
                except Exception as e:
                    continue
            
            self.products = products_found
            return len(products_found) > 0
            
        except Exception as e:
            print(f"Error extracting from search page: {e}")
            return False
    
    def extract_product_data(self, element):
        """Extract data from a single product element"""
        try:
            product_info = {}
            
            # Extract name
            name_selectors = [
                "[data-automation-id='product-title']",
                ".normal.dark-gray.mb0.mt1.lh-title.f6.f5-l",
                "h3",
                ".product-title",
                "[data-testid='product-title']"
            ]
            
            for selector in name_selectors:
                try:
                    name_elem = element.find_element(By.CSS_SELECTOR, selector)
                    product_info['name'] = name_elem.text.strip()
                    break
                except:
                    continue
            
            # Extract price
            price_selectors = [
                "[itemprop='price']",
                "[data-automation-id='product-price']",
                ".price-current",
                ".price-group",
                ".f2.b.dark-gray.lh-copy.f1-xl"
            ]
            
            for selector in price_selectors:
                try:
                    price_elem = element.find_element(By.CSS_SELECTOR, selector)
                    product_info['price'] = price_elem.text.strip()
                    break
                except:
                    continue
            
            # Extract image URL
            try:
                img_elem = element.find_element(By.CSS_SELECTOR, "img")
                product_info['image_url'] = img_elem.get_attribute('src')
            except:
                product_info['image_url'] = 'No image found'
            
            # Extract product URL
            try:
                link_elem = element.find_element(By.CSS_SELECTOR, "a")
                href = link_elem.get_attribute('href')
                if href and not href.startswith('http'):
                    href = "https://www.walmart.com" + href
                product_info['url'] = href
            except:
                product_info['url'] = 'No URL found'
            
            # Only return if we got at least a name
            if product_info.get('name'):
                return product_info
            
        except Exception as e:
            pass
        
        return None
    
    def clean_filename(self, filename):
        """Clean filename for saving"""
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        return filename.strip()[:100]
    
    def download_image(self, img_url, product_name, folder='product_images'):
        """Download product image"""
        try:
            if not img_url or img_url == 'No image found':
                return None
                
            if not os.path.exists(folder):
                os.makedirs(folder)
            
            # Get file extension
            extension = '.jpg'
            if '.' in img_url:
                extension = '.' + img_url.split('.')[-1].split('?')[0]
            
            # Create filename
            clean_name = self.clean_filename(product_name)
            filename = f"{clean_name}{extension}"
            filepath = os.path.join(folder, filename)
            
            # Download
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(img_url, headers=headers, timeout=30)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            print(f"üì∏ Downloaded: {filename}")
            return filepath
            
        except Exception as e:
            print(f"‚ùå Error downloading image for {product_name}: {e}")
            return None
    
    def save_data(self):
        """Save scraped data to files"""
        if not self.products:
            return
        
        # Save to CSV
        with open('burts_bees_products.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['name', 'price', 'url', 'image_url'])
            writer.writeheader()
            writer.writerows(self.products)
        
        # Save to JSON
        with open('burts_bees_products.json', 'w', encoding='utf-8') as f:
            json.dump(self.products, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Saved {len(self.products)} products to files")
    
    def run(self):
        """Main execution method"""
        print("üöÄ Starting advanced Walmart scraper...")
        
        try:
            # Try scraping with search approach
            if self.scrape_with_api_approach():
                print(f"‚úÖ Successfully scraped {len(self.products)} products!")
                
                # Display results
                for i, product in enumerate(self.products, 1):
                    print(f"{i}. {product.get('name', 'No name')} - {product.get('price', 'No price')}")
                
                # Download images
                print("\nüì∏ Downloading images...")
                for product in self.products:
                    if product.get('image_url'):
                        self.download_image(product['image_url'], product.get('name', 'unknown'))
                        time.sleep(1)  # Be respectful
                
                # Save data
                self.save_data()
                
            else:
                print("‚ùå Failed to scrape products")
                
        except Exception as e:
            print(f"‚ùå Error during scraping: {e}")
        
        finally:
            if self.driver:
                self.driver.quit()

def main():
    scraper = AdvancedWalmartScraper()
    scraper.run()

if __name__ == "__main__":
    main()